{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2: Written Assessment (Individual) - Classification Modeling Case Study\n",
    "\n",
    "### Elizabeth Parker Schooley - MSFBA5\n",
    "\n",
    "### March 15, 2020\n",
    "\n",
    "> This report will cover the process of exploratory data analysis of the Apprentice Chef Dataset, as well as chosen feature engineering techniques, and ultimately modeling frameworks chosen. This is a classification predictive model will utilize CROSS_SELL_SUCCESS as the target variables and all remaining variables will initially start-off as potential explanatory variables for the model.\n",
    "\n",
    "> It should be noted that a feature was added to the initial dataset using external research. After confirming all values were duplicates in the name and first_name columns, a column was added indicating if the person was a male or female (1=male, and 0=female). The source of this additional variable can be found following this link: <https://www.kaggle.com/mylesoneill/game-of-thrones#character-predictions.csv> and navigating to the character-predictions.csv. \n",
    "\n",
    "### Section I\n",
    "\n",
    "#### EDA Processes and Feature Engineering\n",
    "> 1. Importing all necessary packages and user defined functions created by Professor Chase Kusterer\n",
    "> 2. Checking the head, type of each variable, and for null values: 47 null values were detected in the Family_Name column\n",
    "> 3. Created a feature to classify by Apprentice Chef's email domain categories by splitting the emails from their domain and classifying as: personal, professional, junk.\n",
    "> 4. Created a feature called Total_Logins, which is the sum of PC_logins and Mobile_logins\n",
    "> 5. Dropped the following non-numerical columns: FAMILY_NAME, NAME, FIRST_NAME, EMAIL, personal_email_domain, domain_group and junk (to avoid dummy variable trap).\n",
    "> 6. Checked for missing values again and confirmed none are present\n",
    "> 7. Took counts of categorical variable groups to confirm n > 75 for each category \n",
    "> 8. Proceed with EDA by generating histograms and boxplots for each variable to observe outliers\n",
    "> 9. Created outlier thresholds with necessary variables as either a low or high threshold \n",
    "> 10. Created feature columns for each outlier threshold \n",
    "> 11. Dataframes with variables and list of labels were created for both the target variable and explanatory variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Package and User-Defined Functions Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import random            as rand                     # random number gen\n",
    "import pandas            as pd                       # data science essentials\n",
    "import matplotlib.pyplot as plt                      # data visualization\n",
    "import seaborn           as sns                      # enhanced data viz\n",
    "from sklearn.model_selection import train_test_split # train-test split\n",
    "from sklearn.linear_model import LogisticRegression  # logistic regression\n",
    "from sklearn.metrics import confusion_matrix         # confusion matrix\n",
    "from sklearn.metrics import roc_auc_score            # auc score\n",
    "from sklearn.neighbors import KNeighborsClassifier   # KNN for classification\n",
    "from sklearn.neighbors import KNeighborsRegressor    # KNN for regression\n",
    "from sklearn.preprocessing import StandardScaler     # standard scaler\n",
    "\n",
    "# libraries for classification trees\n",
    "from sklearn.tree import DecisionTreeClassifier      # classification trees\n",
    "from sklearn.tree import export_graphviz             # exports graphics\n",
    "from sklearn.externals.six import StringIO           # saves objects in memory\n",
    "from IPython.display import Image                    # displays on frontend                                  \n",
    "\n",
    "# Hyperparameters\n",
    "from sklearn.model_selection import GridSearchCV     # hyperparameter tuning\n",
    "from sklearn.metrics import make_scorer              # customizable scorer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all user defined functions\n",
    "\n",
    "########################################\n",
    "# optimal_neighbors\n",
    "########################################\n",
    "def optimal_neighbors(X_data,\n",
    "                      y_data,\n",
    "                      standardize = True,\n",
    "                      pct_test=0.25,\n",
    "                      seed=222,\n",
    "                      response_type='reg',\n",
    "                      max_neighbors=20,\n",
    "                      show_viz=True):\n",
    "    \"\"\"\n",
    "Exhaustively compute training and testing results for KNN across\n",
    "[1, max_neighbors]. Outputs the maximum test score and (by default) a\n",
    "visualization of the results.\n",
    "PARAMETERS\n",
    "----------\n",
    "X_data        : explanatory variable data\n",
    "y_data        : response variable\n",
    "standardize   : whether or not to standardize the X data, default True\n",
    "pct_test      : test size for training and validation from (0,1), default 0.25\n",
    "seed          : random seed to be used in algorithm, default 802\n",
    "response_type : type of neighbors algorithm to use, default 'reg'\n",
    "    Use 'reg' for regression (KNeighborsRegressor)\n",
    "    Use 'class' for classification (KNeighborsClassifier)\n",
    "max_neighbors : maximum number of neighbors in exhaustive search, default 20\n",
    "show_viz      : display or surpress k-neigbors visualization, default True\n",
    "\"\"\"    \n",
    "    \n",
    "    \n",
    "    if standardize == True:\n",
    "        # optionally standardizing X_data\n",
    "        scaler             = StandardScaler()\n",
    "        scaler.fit(X_data)\n",
    "        X_scaled           = scaler.transform(X_data)\n",
    "        X_scaled_df        = pd.DataFrame(X_scaled)\n",
    "        X_data             = X_scaled_df\n",
    "\n",
    "\n",
    "\n",
    "    # train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data,\n",
    "                                                        y_data,\n",
    "                                                        test_size = pct_test,\n",
    "                                                        random_state = seed)\n",
    "\n",
    "\n",
    "    # creating lists for training set accuracy and test set accuracy\n",
    "    training_accuracy = []\n",
    "    test_accuracy = []\n",
    "    \n",
    "    \n",
    "    # setting neighbor range\n",
    "    neighbors_settings = range(1, max_neighbors + 1)\n",
    "\n",
    "\n",
    "    for n_neighbors in neighbors_settings:\n",
    "        # building the model based on response variable type\n",
    "        if response_type == 'reg':\n",
    "            clf = KNeighborsRegressor(n_neighbors = n_neighbors)\n",
    "            clf.fit(X_train, y_train)\n",
    "            \n",
    "        elif response_type == 'class':\n",
    "            clf = KNeighborsClassifier(n_neighbors = n_neighbors)\n",
    "            clf.fit(X_train, y_train)            \n",
    "            \n",
    "        else:\n",
    "            print(\"Error: response_type must be 'reg' or 'class'\")\n",
    "        \n",
    "        \n",
    "        # recording the training set accuracy\n",
    "        training_accuracy.append(clf.score(X_train, y_train))\n",
    "    \n",
    "        # recording the generalization accuracy\n",
    "        test_accuracy.append(clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "    # optionally displaying visualization\n",
    "    if show_viz == True:\n",
    "        # plotting the visualization\n",
    "        fig, ax = plt.subplots(figsize=(12,8))\n",
    "        plt.plot(neighbors_settings, training_accuracy, label = \"training accuracy\")\n",
    "        plt.plot(neighbors_settings, test_accuracy, label = \"test accuracy\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.xlabel(\"n_neighbors\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    # returning optimal number of neighbors\n",
    "    print(f\"The optimal number of neighbors is: {test_accuracy.index(max(test_accuracy))+1}\")\n",
    "    return test_accuracy.index(max(test_accuracy))+1\n",
    "\n",
    "\n",
    "########################################\n",
    "# visual_cm\n",
    "########################################\n",
    "def visual_cm(true_y, pred_y, labels = None):\n",
    "    \"\"\"\n",
    "Creates a visualization of a confusion matrix.\n",
    "\n",
    "PARAMETERS\n",
    "----------\n",
    "true_y : true values for the response variable\n",
    "pred_y : predicted values for the response variable\n",
    "labels : , default None\n",
    "    \"\"\"\n",
    "    # visualizing the confusion matrix\n",
    "\n",
    "    # setting labels\n",
    "    lbls = labels\n",
    "    \n",
    "\n",
    "    # declaring a confusion matrix object\n",
    "    cm = confusion_matrix(y_true = true_y,\n",
    "                          y_pred = pred_y)\n",
    "\n",
    "\n",
    "    # heatmap\n",
    "    sns.heatmap(cm,\n",
    "                annot       = True,\n",
    "                xticklabels = lbls,\n",
    "                yticklabels = lbls,\n",
    "                cmap        = 'Blues',\n",
    "                fmt         = 'g')\n",
    "\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix of the Classifier')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "########################################\n",
    "# display_tree\n",
    "########################################\n",
    "def display_tree(tree, feature_df, height = 500, width = 800):\n",
    "    \"\"\"\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    tree       : fitted tree model object\n",
    "        fitted CART model to visualized\n",
    "    feature_df : DataFrame\n",
    "        DataFrame of explanatory features (used to generate labels)\n",
    "    height     : int, default 500\n",
    "        height in pixels to which to constrain image in html\n",
    "    width      : int, default 800\n",
    "        width in pixels to which to constrain image in html\n",
    "    \"\"\"\n",
    "\n",
    "    # visualizing the tree\n",
    "    dot_data = StringIO()\n",
    "\n",
    "    \n",
    "    # exporting tree to graphviz\n",
    "    export_graphviz(decision_tree      = tree,\n",
    "                    out_file           = dot_data,\n",
    "                    filled             = True,\n",
    "                    rounded            = True,\n",
    "                    special_characters = True,\n",
    "                    feature_names      = feature_df.columns)\n",
    "\n",
    "\n",
    "    # declaring a graph object\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "\n",
    "\n",
    "    # creating image\n",
    "    img = Image(graph.create_png(),\n",
    "                height = height,\n",
    "                width  = width)\n",
    "    \n",
    "    return img\n",
    "\n",
    "########################################\n",
    "# plot_feature_importances\n",
    "########################################\n",
    "def plot_feature_importances(model, train, export = False):\n",
    "    \"\"\"\n",
    "    Plots the importance of features from a CART model.\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    model  : CART model\n",
    "    train  : explanatory variable training data\n",
    "    export : whether or not to export as a .png image, default False\n",
    "    \"\"\"\n",
    "    \n",
    "    # declaring the number\n",
    "    n_features = X_train.shape[1]\n",
    "    \n",
    "    # setting plot window\n",
    "    fig, ax = plt.subplots(figsize=(12,9))\n",
    "    \n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(pd.np.arange(n_features), train.columns)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    \n",
    "    if export == True:\n",
    "        plt.savefig('Tree_Leaf_50_Feature_Importance.png')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading in raw data frame and feature engineering emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting pandas print options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "#loading in raw data file\n",
    "file = 'Apprentice_Chef_Dataset A2.xlsx'\n",
    "chef = pd.read_excel(file)\n",
    "\n",
    "#checking label names\n",
    "#chef.head(n = 5)\n",
    "\n",
    "#checking types of variables\n",
    "#type(chef)\n",
    "\n",
    "#FEATURE ENGINEERING\n",
    "# STEP 1: splitting personal emails\n",
    "\n",
    "# placeholder list\n",
    "placeholder_lst = []\n",
    "\n",
    "# looping over each email address\n",
    "for index, col in chef.iterrows():\n",
    "    \n",
    "    # splitting email domain at '@'\n",
    "    split_email = chef.loc[index, 'EMAIL'].split(sep = '@')\n",
    "    \n",
    "    # appending placeholder_lst with the results\n",
    "    placeholder_lst.append(split_email)\n",
    "    \n",
    "\n",
    "# converting placeholder_lst into a DataFrame \n",
    "email_df = pd.DataFrame(placeholder_lst)\n",
    "\n",
    "# STEP 2: concatenating with original DataFrame\n",
    "\n",
    "# renaming column to concatenate\n",
    "email_df.columns = ['0' , 'personal_email_domain']\n",
    "\n",
    "\n",
    "# concatenating personal_email_domain with chef DataFrame\n",
    "chef = pd.concat([chef, email_df['personal_email_domain']],\n",
    "                     axis = 1)\n",
    "\n",
    "\n",
    "# printing value counts of personal_email_domain\n",
    "chef.loc[: ,'personal_email_domain'].value_counts()\n",
    "\n",
    "# email domain types\n",
    "personal_email_domains = ['@gmail.com', '@yahoo.com', '@protonmail.com']\n",
    "professional_email_domains  = ['@mmm.com', '@amex.com', '@apple.com', '@boeing.com', \n",
    "                               '@caterpillar.com', '@chevron.com', '@cisco.com', \n",
    "                               '@cocacola.com', '@disney.com', '@dupont.com', \n",
    "                               '@exxon.com', '@ge.org', '@goldmansacs.com', '@homedepot.com', \n",
    "                               '@ibm.com', '@intel.com', '@jnj.com', '@jpmorgan.com',\n",
    "                               '@mcdonalds.com', '@merck.com', '@microsoft.com', \n",
    "                               '@nike.com', '@pfizer.com', '@pg.com', '@travelers.com', \n",
    "                               '@unitedtech.com', '@unitedhealth.com', '@verizon.com', \n",
    "                               '@visa.com', '@walmart.com']\n",
    "junk_email_domains = ['@me.com', '@aol.com', '@hotmail.com', '@live.com',\n",
    "                     '@msn.com', '@passport.com']\n",
    "\n",
    "\n",
    "# placeholder list\n",
    "placeholder_lst = []\n",
    "\n",
    "\n",
    "# looping to group observations by domain type\n",
    "for domain in chef['personal_email_domain']:\n",
    "    \n",
    "    if '@' + domain in personal_email_domains:\n",
    "        placeholder_lst.append('personal')\n",
    "        \n",
    "\n",
    "    elif '@' + domain in professional_email_domains:\n",
    "        placeholder_lst.append('professional')\n",
    "        \n",
    "    elif '@' + domain in junk_email_domains:\n",
    "        placeholder_lst.append('junk')\n",
    "\n",
    "\n",
    "    else:\n",
    "            print('Unknown')\n",
    "\n",
    "\n",
    "# concatenating with original DataFrame\n",
    "chef['domain_group'] = pd.Series(placeholder_lst)\n",
    "\n",
    "\n",
    "# checking results\n",
    "chef['domain_group'].value_counts()\n",
    "\n",
    "# one hot encoding categorical variables\n",
    "one_hot_email       = pd.get_dummies(chef['domain_group'])\n",
    "\n",
    "# joining codings together\n",
    "chef = chef.join([one_hot_email])\n",
    "\n",
    "#checking for missing values\n",
    "#chef.isnull().any()\n",
    "\n",
    "#chef.columns \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping Non-Numerical and Non-encoded categorical variables and Creating Outlier Threshold Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping FAMILY_NAME, NAME, FIRST_NAME, EMAIL, personal_email_domain\n",
    "#domain_group and junk (to avoid dummy variable trap)\n",
    "#feature engineered new categorical column with 'male' which tells\n",
    "#whether each first_name is a male or female, \n",
    "#email categories, and total_logins \n",
    "\n",
    "\n",
    "#loading saved file\n",
    "data = chef\n",
    "\n",
    "#creating dataframe with desired variables\n",
    "df = pd.DataFrame(data = data, columns = ['CROSS_SELL_SUCCESS', 'REVENUE', 'male', 'TOTAL_MEALS_ORDERED', 'UNIQUE_MEALS_PURCH', \n",
    "       'CONTACTS_W_CUSTOMER_SERVICE', 'PRODUCT_CATEGORIES_VIEWED', 'AVG_TIME_PER_SITE_VISIT', \n",
    "       'MOBILE_NUMBER', 'CANCELLATIONS_BEFORE_NOON', 'CANCELLATIONS_AFTER_NOON', \n",
    "       'TASTES_AND_PREFERENCES', 'PC_LOGINS', 'MOBILE_LOGINS', 'WEEKLY_PLAN', \n",
    "       'EARLY_DELIVERIES', 'LATE_DELIVERIES', 'PACKAGE_LOCKER', \n",
    "       'REFRIGERATED_LOCKER', 'FOLLOWED_RECOMMENDATIONS_PCT', 'AVG_PREP_VID_TIME', \n",
    "       'LARGEST_ORDER_SIZE', 'MASTER_CLASSES_ATTENDED', 'MEDIAN_MEAL_RATING', \n",
    "       'AVG_CLICKS_PER_VISIT', 'TOTAL_PHOTOS_VIEWED', 'personal', 'professional'])\n",
    "\n",
    "\n",
    "#df.columns\n",
    "\n",
    "\n",
    "#setting outlier flags after visual histogram EDA and box plot EDA \n",
    "#on all variables\n",
    "REVENUE_hi = 3000\n",
    "AVG_TIME_PER_SITE_VISIT_hi = 220\n",
    "AVG_PREP_VID_TIME_hi = 200\n",
    "CONTACTS_W_CUSTOMER_SERVICE_hi = 9\n",
    "AVG_CLICKS_PER_VISIT_lo = 10\n",
    "TOTAL_MEALS_ORDERED_hi  = 106\n",
    "UNIQUE_MEALS_PURCH_hi = 9\n",
    "CANCELLATIONS_BEFORE_NOON_hi = 4\n",
    "WEEKLY_PLAN_hi = 16\n",
    "EARLY_DELIVERIES_hi = 4\n",
    "LATE_DELIVERIES_hi = 5\n",
    "LARGEST_ORDER_SIZE_lo = 3\n",
    "LARGEST_ORDER_SIZE_hi = 6\n",
    "TOTAL_PHOTOS_VIEWED_hi = 220\n",
    "MEDIAN_MEAL_RATING_hi = 4\n",
    "\n",
    "##############################################################################\n",
    "## Feature Engineering (outlier thresholds)                                 ##\n",
    "##############################################################################\n",
    "\n",
    "# developing features (columns) for outliers\n",
    "\n",
    "# Revenue\n",
    "df['out_REVENUE'] = 0\n",
    "condition_hi = df.loc[0:,'out_REVENUE'][df['REVENUE'] > REVENUE_hi]\n",
    "\n",
    "df['out_REVENUE'].replace(to_replace = condition_hi,\n",
    "                                value      = 1,\n",
    "                                inplace    = True)\n",
    "\n",
    "\n",
    "# Average Time Per Site Visit\n",
    "df['out_AVG_TIME_PER_SITE_VISIT'] = 0\n",
    "condition_hi = df.loc[0:,'out_AVG_TIME_PER_SITE_VISIT'][df['AVG_TIME_PER_SITE_VISIT'] > AVG_TIME_PER_SITE_VISIT_hi]\n",
    "\n",
    "df['out_AVG_TIME_PER_SITE_VISIT'].replace(to_replace = condition_hi,\n",
    "                                value      = 1,\n",
    "                                inplace    = True)\n",
    "\n",
    "\n",
    "# Largest Order Size\n",
    "df['out_LARGEST_ORDER_SIZE'] = 0\n",
    "condition_hi = df.loc[0:,'out_LARGEST_ORDER_SIZE'][df['LARGEST_ORDER_SIZE'] > LARGEST_ORDER_SIZE_hi]\n",
    "condition_lo = df.loc[0:,'out_LARGEST_ORDER_SIZE'][df['LARGEST_ORDER_SIZE'] < LARGEST_ORDER_SIZE_lo]\n",
    "\n",
    "df['out_LARGEST_ORDER_SIZE'].replace(to_replace = condition_hi,\n",
    "                                    value      = 1,\n",
    "                                    inplace    = True)\n",
    "\n",
    "df['out_LARGEST_ORDER_SIZE'].replace(to_replace = condition_lo,\n",
    "                                    value      = 1,\n",
    "                                    inplace    = True)\n",
    "\n",
    "\n",
    "# Average Clicks Per Visit\n",
    "df['out_AVG_CLICKS_PER_VISIT'] = 0\n",
    "condition_lo = df.loc[0:,'out_AVG_CLICKS_PER_VISIT'][df['AVG_CLICKS_PER_VISIT'] < AVG_CLICKS_PER_VISIT_lo]\n",
    "\n",
    "df['out_AVG_CLICKS_PER_VISIT'].replace(to_replace = condition_lo,\n",
    "                                    value      = 1,\n",
    "                                    inplace    = True)\n",
    "\n",
    "\n",
    "# Average Prep Video Time\n",
    "df['out_AVG_PREP_VID_TIME'] = 0\n",
    "condition_hi = df.loc[0:,'out_AVG_PREP_VID_TIME'][df['AVG_PREP_VID_TIME'] > AVG_PREP_VID_TIME_hi]\n",
    "\n",
    "df['out_AVG_PREP_VID_TIME'].replace(to_replace = condition_hi,\n",
    "                                    value      = 1,\n",
    "                                    inplace    = True)\n",
    "\n",
    "\n",
    "# Contacts wit Customer Service\n",
    "df['out_CONTACTS_W_CUSTOMER_SERVICE'] = 0\n",
    "condition_hi = df.loc[0:,'out_CONTACTS_W_CUSTOMER_SERVICE'][df['CONTACTS_W_CUSTOMER_SERVICE'] > CONTACTS_W_CUSTOMER_SERVICE_hi]\n",
    "\n",
    "df['out_CONTACTS_W_CUSTOMER_SERVICE'].replace(to_replace = condition_hi,\n",
    "                                    value      = 1,\n",
    "                                    inplace    = True)\n",
    "\n",
    "\n",
    "# Total Meals Ordered\n",
    "df['out_TOTAL_MEALS_ORDERED'] = 0\n",
    "condition_hi = df.loc[0:,'out_TOTAL_MEALS_ORDERED'][df['TOTAL_MEALS_ORDERED'] > TOTAL_MEALS_ORDERED_hi]\n",
    "\n",
    "df['out_TOTAL_MEALS_ORDERED'].replace(to_replace = condition_hi,\n",
    "                                    value      = 1,\n",
    "                                    inplace    = True)\n",
    "\n",
    "\n",
    "# Unique Meals Purchased\n",
    "df['out_UNIQUE_MEALS_PURCH'] = 0\n",
    "condition_hi = df.loc[0:,'out_UNIQUE_MEALS_PURCH'][df['UNIQUE_MEALS_PURCH'] > UNIQUE_MEALS_PURCH_hi]\n",
    "\n",
    "df['out_UNIQUE_MEALS_PURCH'].replace(to_replace = condition_hi,\n",
    "                                 value      = 1,\n",
    "                                 inplace    = True)\n",
    "\n",
    "\n",
    "# Cancellations Before Noon\n",
    "df['out_CANCELLATIONS_BEFORE_NOON'] = 0\n",
    "condition_hi = df.loc[0:,'out_CANCELLATIONS_BEFORE_NOON'][df['CANCELLATIONS_BEFORE_NOON'] > CANCELLATIONS_BEFORE_NOON_hi]\n",
    "\n",
    "df['out_CANCELLATIONS_BEFORE_NOON'].replace(to_replace = condition_hi,\n",
    "                                 value      = 1,\n",
    "                                 inplace    = True)\n",
    "\n",
    "\n",
    "# Weekly Plan\n",
    "df['out_WEEKLY_PLAN'] = 0\n",
    "condition_hi = df.loc[0:,'out_WEEKLY_PLAN'][df['WEEKLY_PLAN'] > WEEKLY_PLAN_hi]\n",
    "\n",
    "df['out_WEEKLY_PLAN'].replace(to_replace = condition_hi,\n",
    "                                 value      = 1,\n",
    "                                 inplace    = True)\n",
    "\n",
    "\n",
    "# Early Deliveries\n",
    "df['out_EARLY_DELIVERIES'] = 0\n",
    "condition_hi = df.loc[0:,'out_EARLY_DELIVERIES'][df['EARLY_DELIVERIES'] > EARLY_DELIVERIES_hi]\n",
    "\n",
    "df['out_EARLY_DELIVERIES'].replace(to_replace = condition_hi,\n",
    "                                 value      = 1,\n",
    "                                 inplace    = True)\n",
    "\n",
    "\n",
    "# Late Deliveries\n",
    "df['out_LATE_DELIVERIES'] = 0\n",
    "condition_hi = df.loc[0:,'out_LATE_DELIVERIES'][df['LATE_DELIVERIES'] > LATE_DELIVERIES_hi]\n",
    "\n",
    "df['out_LATE_DELIVERIES'].replace(to_replace = condition_hi,\n",
    "                                 value      = 1,\n",
    "                                 inplace    = True)\n",
    "\n",
    "\n",
    "# Total Photos Viewed\n",
    "df['out_TOTAL_PHOTOS_VIEWED'] = 0\n",
    "condition_hi = df.loc[0:,'out_TOTAL_PHOTOS_VIEWED'][df['TOTAL_PHOTOS_VIEWED'] > TOTAL_PHOTOS_VIEWED_hi]\n",
    "\n",
    "df['out_TOTAL_PHOTOS_VIEWED'].replace(to_replace = condition_hi,\n",
    "                                 value      = 1,\n",
    "                                 inplace    = True)\n",
    "\n",
    "\n",
    "# Median Meal Rating\n",
    "df['out_MEDIAN_MEAL_RATING'] = 0\n",
    "condition_hi = df.loc[0:,'out_MEDIAN_MEAL_RATING'][df['MEDIAN_MEAL_RATING'] > MEDIAN_MEAL_RATING_hi]\n",
    "\n",
    "df['out_MEDIAN_MEAL_RATING'].replace(to_replace = condition_hi,\n",
    "                                 value      = 1,\n",
    "                                 inplace    = True)\n",
    "\n",
    "# New feature total logins (PC logins + mobile logins)\n",
    "df['total_logins'] = df['PC_LOGINS']+df['MOBILE_LOGINS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Section II\n",
    "\n",
    "#### Modeling Techniques\n",
    "\n",
    "> 1. Examined Pearson Correlation Coefficients for each explanatory variable with the response variable:\n",
    "\n",
    "####   The Top 7 Correlated Variables with CROSS_SELL_SUCCESS:\n",
    "                FOLLOWED_RECOMMENDATIONS_PCT       0.46\n",
    "                professional                       0.19\n",
    "                CANCELLATIONS_BEFORE_NOON          0.16\n",
    "                MOBILE_NUMBER                      0.10\n",
    "                TASTES_AND_PREFERENCES             0.08\n",
    "                REFRIGERATED_LOCKER                0.07\n",
    "                out_CANCELLATIONS_BEFORE_NOON      0.06\n",
    "> 2. Created a full logit model with smf.logit including all explanatory variables\n",
    "> 3. Defined the list below (df_sig) containing independent variables with P-values < 0.05, with the exception of REFRIGERATED LOCKER, which initially had a P-value of 0.091, but upon developing multiple models with only the significant variables, REFRIGERATED_LOCKER continued to be significant \n",
    "> 4. A second model was developed with only significant variables from the full logit model with a Current Function Value = 0.436\n",
    "> 5. Implemented test, train, split with a test size of 0.25 and random state of 222, and the stratify argument set to 'True' upon reviewing samples with CROSS_SELL_SUCCESS = 1 and CROSS_SELL_SUCCESS = 0 \n",
    "> 6. Upon reviewing the descriptive statistics of each variable and quantile plot, it was decided to implement StandardScaler to scale all features in an effort to consider all features uniformly \n",
    "> 7. A Logistic Regression classifier was instantiated and run on the scaled significant explanatory variables \n",
    "> 8. Model was fit on the X and Y training data and predicted results X test data to produce a Training Accuracy = 0.763 and Testing Accuracy = 0.762\n",
    "> 9. Confusion Matrix of the Classifier was created and area under the ROC curve was calculated and = 0.706\n",
    "> 10. The optimal number of neighbors was found using the user-defined function provided above in order to instantiate the KNN classification model\n",
    "> 11. The KNN classification model was fitted on the previously scaled explanatory variables the model performance is summarized below:\n",
    "\n",
    "                > Training ACCURACY: 0.806\n",
    "                  Testing  ACCURACY: 0.7228\n",
    "                  AUC Score        : 0.7063\n",
    "> 12. A Decision Tree Classifier was instantiated on all non-scaled explanatory variables. After testing, training, splitting, fitting, and predicting the Full Tree Model, the results were produced below:\n",
    "\n",
    "                 > Training ACCURACY: 0.862\n",
    "                   Testing  ACCURACY: 0.77\n",
    "                   AUC Score        : 0.7512\n",
    "                   \n",
    "> 13. To prevent overfitting, a second pruned Decision Tree Classifier was instantiated and fit on the following set of variables identified as significant features from the original logit model: \n",
    "\n",
    "      > 'MOBILE_NUMBER', \n",
    "        'CANCELLATIONS_BEFORE_NOON',\n",
    "        'TASTES_AND_PREFERENCES',\n",
    "        'FOLLOWED_RECOMMENDATIONS_PCT',\n",
    "        'personal',\n",
    "        'professional',\n",
    "        'out_CANCELLATIONS_BEFORE_NOON',\n",
    "        'REFRIGERATED_LOCKER'\n",
    "> 14. These features were scaled and the pruned Decision Tree Classifier produced the following results: \n",
    "\n",
    "                    > Training ACCURACY: 0.8033\n",
    "                      Testing  ACCURACY: 0.7926\n",
    "                      AUC Score        : 0.7763\n",
    "                      \n",
    "> 15. A bar chart of the Pruned Decision Tree's Important Features was created identifying the following features as most important:\n",
    "\n",
    "           > 1. FOLLOWED_RECOMMENDATIONS_PCT (approximately 0.8)\n",
    "             2. professional\n",
    "             3. personal\n",
    "             4. CANCELLATIONS_BEFORE_NOON \n",
    "             5. MOBILE_NUMBER\n",
    "        * all features following feature 1 were below 0.2\n",
    "> 16. After completing testing, training, predicting and fitting using scaled, unscaled, all explanatory variables, and only initially identified explanatory variables and instantiating KNN Classifier Model, Logit Model, Logistic Regression Classifier Model, Decision Tree Unpruned and Pruned Classifier Models, and Random Forest Classifier Model, the bet performing model was created and can be seen below:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section III\n",
    "\n",
    "#### Best Performing Modeling Technique and Hyperparameter Tuning\n",
    "\n",
    "> The model performing the highest in it's AUC score utilized the significant variables identified throughout my iterative modeling technique processes. The significant explanatory variables in the final model were: \n",
    "\n",
    "    > FINAL MODEL EXPLANATORY VARIABLES:\n",
    "        'MOBILE_NUMBER', \n",
    "        'CANCELLATIONS_BEFORE_NOON',\n",
    "        'TASTES_AND_PREFERENCES',\n",
    "        'FOLLOWED_RECOMMENDATIONS_PCT',\n",
    "        'personal',\n",
    "        'professional',\n",
    "        'out_CANCELLATIONS_BEFORE_NOON',\n",
    "        'REFRIGERATED_LOCKER'\n",
    "        \n",
    "#### Final Model Process\n",
    "> 1. Explanatory variables were scaled, trained, tested, and spit with a test size = 0.25 and random state = 222\n",
    "> 2. Hyperparameter space was declared and a GridSearchCV object was created to automate the search for the optimal hyperparameters\n",
    "> 3. The Tuned Parameters were found to be: 'criterion'= 'entropy', 'max_depth'= 5, 'min_samples_leaf'= 2, 'splitter'= 'best'\n",
    "> 4. A Logistic Regression Model with tuned values was instantiated using lr_tuned_cv.best_estimator_\n",
    "> 5. This model yielded the following results:\n",
    "\n",
    "                            > Training ACCURACY: 0.8177\n",
    "                              Testing  ACCURACY: 0.8131\n",
    "                              AUC Score        : 0.7829\n",
    "> 6. The Tuned Tree important features were plotted for this model, identifying most important features as: \n",
    "\n",
    "         > 1. FOLLOWED_RECOMMENDATIONS_PCT (approximately 0.8)\n",
    "           2. professional\n",
    "           3. personal\n",
    "           4. CANCELLATIONS_BEFORE_NOON \n",
    "           5. MOBILE_NUMBER\n",
    "           6. REFRIGERATED_LOCKER\n",
    "           7. TASTES AND PREFERENCES\n",
    "        * all features following feature 1 were below 0.2\n",
    "        * features 6 and 7 were approximately equal in importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining explanatory variables and target\n",
    "xVar = list(df.loc[:,'REVENUE':'professional'])\n",
    "df2 = df[xVar]\n",
    "yVar = df.iloc[:,0]\n",
    "\n",
    "#defining significant variables after full logistic regression run\n",
    "df_sig = df[['MOBILE_NUMBER', 'CANCELLATIONS_BEFORE_NOON',\n",
    "                                        'TASTES_AND_PREFERENCES',\n",
    "                                        'FOLLOWED_RECOMMENDATIONS_PCT',\n",
    "                                        'personal',\n",
    "                                        'professional',\n",
    "                                        'out_CANCELLATIONS_BEFORE_NOON',\n",
    "                                        'REFRIGERATED_LOCKER']]\n",
    "\n",
    "# train/test split with the logit_sig variables\n",
    "df_data   =  df_sig\n",
    "df_target =  yVar\n",
    "\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            df_data,\n",
    "            df_target,\n",
    "            random_state = 222,\n",
    "            test_size    = 0.25,\n",
    "            stratify     = df_target)\n",
    "\n",
    "\n",
    "# INSTANTIATING StandardScaler()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# FITTING the data\n",
    "scaler.fit(df_data)\n",
    "\n",
    "\n",
    "# TRANSFORMING the data\n",
    "X_scaled     = scaler.transform(df_data)\n",
    "\n",
    "\n",
    "# converting to a DataFrame\n",
    "X_scaled_df  = pd.DataFrame(X_scaled) \n",
    "\n",
    "\n",
    "# train-test split with the scaled data\n",
    "X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(\n",
    "            X_scaled_df,\n",
    "            df_target,\n",
    "            random_state = 222,\n",
    "            test_size = 0.25,\n",
    "            stratify = df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring a hyperparameter space\n",
    "#criterion_space = ['gini', 'entropy']\n",
    "#splitter_space = ['best', 'random']\n",
    "#depth_space = pd.np.arange(1, 25)\n",
    "#leaf_space  = pd.np.arange(1, 100)\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "#param_grid = {'criterion'        : criterion_space,\n",
    "#              'splitter'         : splitter_space,\n",
    "#              'max_depth'        : depth_space,\n",
    "#              'min_samples_leaf' : leaf_space}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "#tuned_tree = DecisionTreeClassifier(random_state = 222)\n",
    "\n",
    "\n",
    "# GridSearchCV object\n",
    "#tuned_tree_cv = GridSearchCV(estimator  = tuned_tree,\n",
    "#                             param_grid = param_grid,\n",
    "#                             cv         = 3,\n",
    "#                             scoring    = make_scorer(roc_auc_score,\n",
    "#                                                      needs_threshold = False))\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "#tuned_tree_cv.fit(df_data, df_target)\n",
    "\n",
    "\n",
    "# PREDICT step is not needed\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "#print(\"Tuned Parameters  :\", tuned_tree_cv.best_params_)\n",
    "#print(\"Tuned Training AUC:\", tuned_tree_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.8191\n",
      "Testing  ACCURACY: 0.7864\n",
      "AUC Score        : 0.7683\n"
     ]
    }
   ],
   "source": [
    "# building a model based on hyperparameter tuning results\n",
    "# INSTANTIATING a Decision Tree Classifier model with tuned values\n",
    "tree_tuned  = DecisionTreeClassifier(criterion = 'entropy',\n",
    "                                             max_depth = 5,\n",
    "                                              min_samples_leaf = 2,\n",
    "                                             splitter = 'best',\n",
    "                                              random_state = 222)\n",
    "#FITTING the training data\n",
    "tree_tuned_fit  = tree_tuned.fit(X_train, y_train)\n",
    "\n",
    "# PREDICTING on new data\n",
    "tree_pred = tree_tuned_fit.predict(X_test)\n",
    "    \n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', tree_tuned.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', tree_tuned.score(X_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                              y_score = tree_pred).round(4))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the tree\n",
    "#display_tree(tree = tree_tuned,\n",
    "             #feature_df = df_data,\n",
    "             #height = 2000,\n",
    "             #width  = 2000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:198: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting feature importance\n",
    "plot_feature_importances(tree_tuned_fit,\n",
    "                         train = X_train,\n",
    "                         export = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
